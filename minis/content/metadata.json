[
  {
    "id": "mini200820252002",
    "title": "The First of Many",
    "date": "2025-08-20",
    "time": "20:02",
    "tags": [
      "blog",
      "first",
      "project"
    ],
    "content": "<p><hr></p><p>The wait is finally over!  </p><p>After 2 weeks, and roughly 12 hours of development — it’s here. I present to you <strong>Minis by <em>Beyond Me Btw</em></strong>.  </p><p>A live blogging platform where I can share quick updates and thoughts that don’t need a fully produced blog post. It could be a small discovery in tech, a movie review I didn’t enjoy, or just something I feel like talking about.  </p><p>With Minis, I can give live commentary on projects without waiting until the very end.  </p><p>And this is just the start. I’ve got more features planned, maybe even a Twitter bot for fun, and plenty more to come.  </p><p><strong>Stay Tuned, The best is yet to come!</strong></p><p><img src=\"https://content.beyondmebtw.com/experience/exp30.webp\" alt=\"Minis by Beyond Me Btw\" style=\"max-width: 100%; height: auto;\"></p>",
    "rawMarkdown": "# The First of Many\n\n---\n\nThe wait is finally over!  \n\nAfter 2 weeks, and roughly 12 hours of development — it’s here. I present to you **Minis by *Beyond Me Btw* **.  \n\nA live blogging platform where I can share quick updates and thoughts that don’t need a fully produced blog post. It could be a small discovery in tech, a movie review I didn’t enjoy, or just something I feel like talking about.  \n\nWith Minis, I can give live commentary on projects without waiting until the very end.  \n\nAnd this is just the start. I’ve got more features planned, maybe even a Twitter bot for fun, and plenty more to come.  \n\n**Stay Tuned, The best is yet to come!**\n\n![Minis by Beyond Me Btw](https://content.beyondmebtw.com/experience/exp30.webp)"
  },
  {
    "id": "mini210820252159",
    "title": "Triple Threat",
    "date": "2025-08-21",
    "time": "21:59",
    "tags": [
      "movie",
      "movie review"
    ],
    "content": "<p><hr></p><p>I ended up watching <em>Venom 3: The Last Dance</em>, <em>Kraven the Hunter</em>, and <em>Madame Web</em> back to back. Venom was fine, but the other two, oh my god. I’d heard the talk about how bad they were when they first released, but seeing them myself was something else entirely. Some of them don’t even warrant full reviews, so here are three short ones instead.</p><p><hr></p><p><strong>Venom: The Last Dance (2024)</strong><br>It’s dumb, but it’s sweet and somehow that’s exactly why it works. Probably the only Sony Spider-Man villain to actually make it through a full trilogy, Venom goes out on a surprisingly fun and heartfelt note. The plot is decent, a little messy in places, but never boring. The dancing scene with Mrs. Chen? Adorable. And Eddie saying goodbye to Venom with <em>Memories</em> by Maroon 5 playing in the background unintentionally hilarious, but still kind of touching. The Venom trilogy might and will be Sony’s best work: fun, ridiculous, but never dull. <strong>10/10</strong></p><p><hr></p><p><strong>Kraven the Hunter (2024)</strong><br>Not even “fun bad,” just bad. Kraven feels like a Russian Tarzan knockoff, and not a good one. The twists are dumb, the characters are weak, and the whole thing drags on until you stop caring. Even cool-sounding characters like Chameleon, Foreigner, and Rhino are wasted and reduced to jokes. Completely forgettable. <strong>1/10</strong></p><p><hr></p><p><strong>Madame Web (2024)</strong><br>I made the mistake of watching this right after <em>Kraven the Hunter</em> and honestly, it made <em>Kraven</em> look like a masterpiece in comparison. This isn’t just boring, it’s boring <em>and</em> dumb. The plot is a mess, the dialogue is weak and action was so bad that a kid smashing action figures together would’ve looked better. You can’t help but wonder if this thing was made as a tax write-off. Worst of all, the cast deserved better they were probably told this was going to be the next big Marvel event, only for it to be the next big joke. <strong>1/10</strong></p><p><hr></p><p>Well, that was that. Six hours in total, and only two of them were actually fun. Skip <em>Kraven</em> and <em>Madame Web</em>, just watch <em>Venom 3</em></p>",
    "rawMarkdown": "I ended up watching *Venom 3: The Last Dance*, *Kraven the Hunter*, and *Madame Web* back to back. Venom was fine, but the other two, oh my god. I’d heard the talk about how bad they were when they first released, but seeing them myself was something else entirely. Some of them don’t even warrant full reviews, so here are three short ones instead.\n\n---\n\n**Venom: The Last Dance (2024)**  \nIt’s dumb, but it’s sweet and somehow that’s exactly why it works. Probably the only Sony Spider-Man villain to actually make it through a full trilogy, Venom goes out on a surprisingly fun and heartfelt note. The plot is decent, a little messy in places, but never boring. The dancing scene with Mrs. Chen? Adorable. And Eddie saying goodbye to Venom with *Memories* by Maroon 5 playing in the background unintentionally hilarious, but still kind of touching. The Venom trilogy might and will be Sony’s best work: fun, ridiculous, but never dull. **10/10**\n\n---\n\n**Kraven the Hunter (2024)**  \nNot even “fun bad,” just bad. Kraven feels like a Russian Tarzan knockoff, and not a good one. The twists are dumb, the characters are weak, and the whole thing drags on until you stop caring. Even cool-sounding characters like Chameleon, Foreigner, and Rhino are wasted and reduced to jokes. Completely forgettable. **1/10**\n\n---\n\n**Madame Web (2024)**  \nI made the mistake of watching this right after *Kraven the Hunter* and honestly, it made *Kraven* look like a masterpiece in comparison. This isn’t just boring, it’s boring *and* dumb. The plot is a mess, the dialogue is weak and action was so bad that a kid smashing action figures together would’ve looked better. You can’t help but wonder if this thing was made as a tax write-off. Worst of all, the cast deserved better they were probably told this was going to be the next big Marvel event, only for it to be the next big joke. **1/10**\n\n---\n\nWell, that was that. Six hours in total, and only two of them were actually fun. Skip *Kraven* and *Madame Web*, just watch *Venom 3*"
  },
  {
    "id": "mini020920252144",
    "title": "A Stand Still : Lumex Project",
    "date": "2025-09-02",
    "time": "21:44",
    "tags": [
      "project",
      "computer vision",
      "ai"
    ],
    "content": "<p>Today I was working on my big project, a multimodal AI application. For this, I’m using a Luxonis OAK-D Pro, which combines a depth sensor and a camera. The idea is simple: it provides more data than a regular camera, which is why I bought one in the first place.</p><p>But the title <em>“stand still”</em> sums up exactly where I am right now.</p><p>Last week, I thought I was routing the video stream directly to my phone. In reality, it was going from the camera → PC → phone. My first goal was a proper round trip, but I’ve hit a literal stand still right at the beginning.</p><p>After digging deeper, I found out that I couldn’t just use a web server or web app to connect. The OAK-D needs a <strong>physical connection</strong> between the app and the camera. I looked into some GitHub repos that supposedly did this, but they’re all 4+ years old and broken. I even tried Chaquopy and a bunch of random approaches, hoping something would stick.</p><p>Finally, I stripped it all down and aimed for the bare minimum: just get the RGB stream working. After about three hours, I managed to build an app that _ran without crashing_. But it still couldn’t connect to the camera, and that’s where my progress stands.</p><p>Talking it over with others made me realize something important: I’ve been shooting in the dark and just hoping things would work. That’s not uncommon, especially with how much AI and LLMs can “fill in the blanks” for you. But at some point, the shortcuts collapse.</p><p>Now I see the real path forward: I need to break this down even further. I have to understand the <strong>protocols, pipelines, and data types</strong> at play. Only then will I actually be able to make this work.</p><p>All of the the failing, the learning, the breaking down happened in a single day. But I know I won’t feel fine until I get it running. For clarity (and maybe some sanity), I will keep updating my progress on Minis.</p>",
    "rawMarkdown": "Today I was working on my big project, a multimodal AI application. For this, I’m using a Luxonis OAK-D Pro, which combines a depth sensor and a camera. The idea is simple: it provides more data than a regular camera, which is why I bought one in the first place.\n\nBut the title *“stand still”* sums up exactly where I am right now.\n\nLast week, I thought I was routing the video stream directly to my phone. In reality, it was going from the camera → PC → phone. My first goal was a proper round trip, but I’ve hit a literal stand still right at the beginning.\n\nAfter digging deeper, I found out that I couldn’t just use a web server or web app to connect. The OAK-D needs a **physical connection** between the app and the camera. I looked into some GitHub repos that supposedly did this, but they’re all 4+ years old and broken. I even tried Chaquopy and a bunch of random approaches, hoping something would stick.\n\nFinally, I stripped it all down and aimed for the bare minimum: just get the RGB stream working. After about three hours, I managed to build an app that _ran without crashing_. But it still couldn’t connect to the camera, and that’s where my progress stands.\n\nTalking it over with others made me realize something important: I’ve been shooting in the dark and just hoping things would work. That’s not uncommon, especially with how much AI and LLMs can “fill in the blanks” for you. But at some point, the shortcuts collapse.\n\nNow I see the real path forward: I need to break this down even further. I have to understand the **protocols, pipelines, and data types** at play. Only then will I actually be able to make this work.\n\nAll of the the failing, the learning, the breaking down happened in a single day. But I know I won’t feel fine until I get it running. For clarity (and maybe some sanity), I will keep updating my progress on Minis."
  },
  {
    "id": "mini030920252254",
    "title": "A Realization : Lumex Project",
    "date": "2025-09-03",
    "time": "22:54",
    "tags": [
      "project",
      "computer vision",
      "ai",
      "software"
    ],
    "content": "<p>This is a continuation of me working on my big project, and I’ve made some progress. I broke things down to the core basics of what I’m actually trying to do. After some deep digging (and a stressful back-and-forth with AI), I realized that the repo I was relying on which I already knew was 4 years old, was simply too flawed to sit and fix. I tried starting from scratch, but then I hit the real issue: <strong>AI itself.</strong></p><p>AI is good, no doubt. Without it, I wouldn’t have even thought of this project, let alone gotten this far without a complete breakdown. But the problem is, it just kept giving me the same outdated suggestions again and again.</p><p>If there’s one thing this project has taught me so far, it’s how much software can make your life miserable. Documentation is scarce, libraries change, support is thin, and when you’re trying to build something that hasn’t really been done before, it’s brutally difficult. The panic-filled sleeps and the dreams of failing have been haunting me since Sunday (31/8/25). Talking things over with my father, my accomplishments and failures alike has helped. Even if the problem doesn’t get solved, just talking about it helps. There’s really no one else I can share this with, so I’m grateful I at least have that.</p><p>Now, onto the progress. After going in circles, I finally snapped and told ChatGPT straight: <em>“I’m running in loops here. I’m doing the same thing again and again. All your references are outdated — 4 years old. Existing libraries don’t work due to changes. Stop giving me the same stuff and let’s actually think through how this can work.”</em></p><p>It paused for over four minutes and finally gave me the right direction. The solution: there <strong>does</strong> exist a Java API for DepthAI. It is : <a href='https://github.com/bytedeco/javacpp-presets/tree/master/depthai'>JAVA CPP DEPTHAI Preset</a> It was last updated just 10 months ago, and seeing it still being used in 2024 is a good sign. From there, I broke things down even further. Now my current version is basically just checking if the device is connected no OpenCV, nothing extra, just DepthAI.</p><p>After some back and forth with settings (and still battling the AI feeding me old information), I managed to create an APK that actually compiled the DepthAI library and built successfully. The only issue is that it was built under the assumption that the PC, phone, and OAK were all connected together. The APK ended up being around 400 MB, which isn’t a big deal, but now the next step is to strip it down: build a minimal UI and get this raw functionality working.</p><p>For now, I’ve had to pause because of some external device issues, but I’ll get back to it soon. In the meantime, I want to focus, reflect, and most importantly get my latest F1 recap out.</p><p>And that’s where my progress stands. If nothing else, I’ve already learned that frustration and failure are just part of the process. The important thing is that I’m still moving, even when it feels like I’m standing still.</p>",
    "rawMarkdown": "This is a continuation of me working on my big project, and I’ve made some progress. I broke things down to the core basics of what I’m actually trying to do. After some deep digging (and a stressful back-and-forth with AI), I realized that the repo I was relying on which I already knew was 4 years old, was simply too flawed to sit and fix. I tried starting from scratch, but then I hit the real issue: **AI itself.**\n\nAI is good, no doubt. Without it, I wouldn’t have even thought of this project, let alone gotten this far without a complete breakdown. But the problem is, it just kept giving me the same outdated suggestions again and again.\n\nIf there’s one thing this project has taught me so far, it’s how much software can make your life miserable. Documentation is scarce, libraries change, support is thin, and when you’re trying to build something that hasn’t really been done before, it’s brutally difficult. The panic-filled sleeps and the dreams of failing have been haunting me since Sunday (31/8/25). Talking things over with my father, my accomplishments and failures alike has helped. Even if the problem doesn’t get solved, just talking about it helps. There’s really no one else I can share this with, so I’m grateful I at least have that.\n\nNow, onto the progress. After going in circles, I finally snapped and told ChatGPT straight: *“I’m running in loops here. I’m doing the same thing again and again. All your references are outdated — 4 years old. Existing libraries don’t work due to changes. Stop giving me the same stuff and let’s actually think through how this can work.”*\n\nIt paused for over four minutes and finally gave me the right direction. The solution: there **does** exist a Java API for DepthAI. It is : (JAVA CPP DEPTHAI Preset)[https://github.com/bytedeco/javacpp-presets/tree/master/depthai] It was last updated just 10 months ago, and seeing it still being used in 2024 is a good sign. From there, I broke things down even further. Now my current version is basically just checking if the device is connected no OpenCV, nothing extra, just DepthAI.\n\nAfter some back and forth with settings (and still battling the AI feeding me old information), I managed to create an APK that actually compiled the DepthAI library and built successfully. The only issue is that it was built under the assumption that the PC, phone, and OAK were all connected together. The APK ended up being around 400 MB, which isn’t a big deal, but now the next step is to strip it down: build a minimal UI and get this raw functionality working.\n\nFor now, I’ve had to pause because of some external device issues, but I’ll get back to it soon. In the meantime, I want to focus, reflect, and most importantly get my latest F1 recap out.\n\nAnd that’s where my progress stands. If nothing else, I’ve already learned that frustration and failure are just part of the process. The important thing is that I’m still moving, even when it feels like I’m standing still."
  },
  {
    "id": "mini11092025",
    "title": "Decent Progress : Lumex Project",
    "date": "2025-09-11",
    "time": "21:17",
    "tags": [
      "project",
      "computer vision",
      "ai",
      "software"
    ],
    "content": "<p><strong>Progress Update</strong><br>Over the past couple of days, I focused on getting the DepthAI AAR properly configured and integrated. For context, the AAR is essentially a packaged library (like a zip file) that contains the critical components of the DepthAI module, including the firmware and essential functionality.</p><p>Previously, I discovered that the Bytedeco C++ library worked with DepthAI but only supported the camera itself. not the USB connection. The publicly available AAR only supports USB. This led me to create a <strong>custom AAR</strong> that works for both the camera and USB.</p><p>The AAR build is now <strong>successful</strong>. Initially, there were compilation issues where only a single class file was being compiled while the others were skipped. After cleaning the Gradle cache, double-checking syntax, and following some debugging references, all class files are now correctly included in the AAR.</p><p>With the new AAR integrated, I managed to reach a working APK where both the USB connection and DepthAI function properly. Logs now show <strong>“OAK connected”</strong> and <strong>“Pipeline started”</strong>, confirming that real communication is happening between the phone and the device and no dummy test is happening.</p><p><hr></p><p><strong>Current Challenge: RGB Streaming</strong><br>The next milestone is streaming RGB video to the phone. However, I ran into issues. likely due to a firmware mismatch. While communication works, the streaming pipeline isn’t functioning correctly. I attempted fixes but ended up in loops, so I’ve paused this part for now.</p><p>Moving forward, I am rolling back to the <strong>pipeline-check checkpoint</strong> and rebuild from there, ensuring stability before moving onto streaming. I’m also moving the project to <strong>IntelliJ</strong>, as it’s suggested to be more efficient for this workflow compared to Android Studio. Some configuration issues came up, but I’m confident they’ll be resolved soon.</p><p><hr></p><p><h3><strong>Reflections and Learnings</strong></h3></p><p>It’s been a week since I started this intensive phase of the project. effectively my second week. Progress isn’t measured by speed alone, the issues I’m encountering would take teams to resolve. Fortunately, I’m not alone in this my father has been assisting me, offering 35 years of software development experience. Talking to him gives me perspective and motivation when things feel overwhelming.</p><p>Mentally, I’m stable, but there are moments of fear and self-doubt that made it hard to open Android Studio some days. Even when I make progress, my heart races, and I feel momentarily lost. Yet, I’m still moving forward, which feels like a core part of me saying, _“The only way forward is through.”_</p><p>This project is changing me in ways I don’t fully understand yet. I know that taking breaks is important, but equally important is coming back from them with determination, rather than letting fear take hold. Despite challenges, I feel a quiet confidence that this journey is worth every bit of effort.</p>",
    "rawMarkdown": "**Progress Update**  \nOver the past couple of days, I focused on getting the DepthAI AAR properly configured and integrated. For context, the AAR is essentially a packaged library (like a zip file) that contains the critical components of the DepthAI module, including the firmware and essential functionality.\n\nPreviously, I discovered that the Bytedeco C++ library worked with DepthAI but only supported the camera itself. not the USB connection. The publicly available AAR only supports USB. This led me to create a **custom AAR** that works for both the camera and USB.\n\nThe AAR build is now **successful**. Initially, there were compilation issues where only a single class file was being compiled while the others were skipped. After cleaning the Gradle cache, double-checking syntax, and following some debugging references, all class files are now correctly included in the AAR.\n\nWith the new AAR integrated, I managed to reach a working APK where both the USB connection and DepthAI function properly. Logs now show **“OAK connected”** and **“Pipeline started”**, confirming that real communication is happening between the phone and the device and no dummy test is happening.\n\n---\n\n**Current Challenge: RGB Streaming**  \nThe next milestone is streaming RGB video to the phone. However, I ran into issues. likely due to a firmware mismatch. While communication works, the streaming pipeline isn’t functioning correctly. I attempted fixes but ended up in loops, so I’ve paused this part for now.\n\nMoving forward, I am rolling back to the **pipeline-check checkpoint** and rebuild from there, ensuring stability before moving onto streaming. I’m also moving the project to **IntelliJ**, as it’s suggested to be more efficient for this workflow compared to Android Studio. Some configuration issues came up, but I’m confident they’ll be resolved soon.\n\n---\n\n### **Reflections and Learnings**\n\nIt’s been a week since I started this intensive phase of the project. effectively my second week. Progress isn’t measured by speed alone, the issues I’m encountering would take teams to resolve. Fortunately, I’m not alone in this my father has been assisting me, offering 35 years of software development experience. Talking to him gives me perspective and motivation when things feel overwhelming.\n\nMentally, I’m stable, but there are moments of fear and self-doubt that made it hard to open Android Studio some days. Even when I make progress, my heart races, and I feel momentarily lost. Yet, I’m still moving forward, which feels like a core part of me saying, _“The only way forward is through.”_\n\nThis project is changing me in ways I don’t fully understand yet. I know that taking breaks is important, but equally important is coming back from them with determination, rather than letting fear take hold. Despite challenges, I feel a quiet confidence that this journey is worth every bit of effort."
  }
]
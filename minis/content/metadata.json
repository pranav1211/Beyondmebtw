[
  {
    "id": "mini200820252002",
    "title": "The First of Many",
    "date": "2025-08-20",
    "time": "20:02",
    "tags": [
      "blog",
      "first",
      "project"
    ],
    "content": "<p><hr></p><p>The wait is finally over!  </p><p>After 2 weeks, and roughly 12 hours of development ‚Äî it‚Äôs here. I present to you <strong>Minis by <em>Beyond Me Btw</em></strong>.  </p><p>A live blogging platform where I can share quick updates and thoughts that don‚Äôt need a fully produced blog post. It could be a small discovery in tech, a movie review I didn‚Äôt enjoy, or just something I feel like talking about.  </p><p>With Minis, I can give live commentary on projects without waiting until the very end.  </p><p>And this is just the start. I‚Äôve got more features planned, maybe even a Twitter bot for fun, and plenty more to come.  </p><p><strong>Stay Tuned, The best is yet to come!</strong></p><p><img src=\"https://content.beyondmebtw.com/experience/exp30.webp\" alt=\"Minis by Beyond Me Btw\" style=\"max-width: 100%; height: auto;\"></p>",
    "rawMarkdown": "# The First of Many\n\n---\n\nThe wait is finally over!  \n\nAfter 2 weeks, and roughly 12 hours of development ‚Äî it‚Äôs here. I present to you **Minis by *Beyond Me Btw* **.  \n\nA live blogging platform where I can share quick updates and thoughts that don‚Äôt need a fully produced blog post. It could be a small discovery in tech, a movie review I didn‚Äôt enjoy, or just something I feel like talking about.  \n\nWith Minis, I can give live commentary on projects without waiting until the very end.  \n\nAnd this is just the start. I‚Äôve got more features planned, maybe even a Twitter bot for fun, and plenty more to come.  \n\n**Stay Tuned, The best is yet to come!**\n\n![Minis by Beyond Me Btw](https://content.beyondmebtw.com/experience/exp30.webp)"
  },
  {
    "id": "mini210820252159",
    "title": "Triple Threat",
    "date": "2025-08-21",
    "time": "21:59",
    "tags": [
      "movie",
      "movie review"
    ],
    "content": "<p><hr></p><p>I ended up watching <em>Venom 3: The Last Dance</em>, <em>Kraven the Hunter</em>, and <em>Madame Web</em> back to back. Venom was fine, but the other two, oh my god. I‚Äôd heard the talk about how bad they were when they first released, but seeing them myself was something else entirely. Some of them don‚Äôt even warrant full reviews, so here are three short ones instead.</p><p><hr></p><p><strong>Venom: The Last Dance (2024)</strong><br>It‚Äôs dumb, but it‚Äôs sweet and somehow that‚Äôs exactly why it works. Probably the only Sony Spider-Man villain to actually make it through a full trilogy, Venom goes out on a surprisingly fun and heartfelt note. The plot is decent, a little messy in places, but never boring. The dancing scene with Mrs. Chen? Adorable. And Eddie saying goodbye to Venom with <em>Memories</em> by Maroon 5 playing in the background unintentionally hilarious, but still kind of touching. The Venom trilogy might and will be Sony‚Äôs best work: fun, ridiculous, but never dull. <strong>10/10</strong></p><p><hr></p><p><strong>Kraven the Hunter (2024)</strong><br>Not even ‚Äúfun bad,‚Äù just bad. Kraven feels like a Russian Tarzan knockoff, and not a good one. The twists are dumb, the characters are weak, and the whole thing drags on until you stop caring. Even cool-sounding characters like Chameleon, Foreigner, and Rhino are wasted and reduced to jokes. Completely forgettable. <strong>1/10</strong></p><p><hr></p><p><strong>Madame Web (2024)</strong><br>I made the mistake of watching this right after <em>Kraven the Hunter</em> and honestly, it made <em>Kraven</em> look like a masterpiece in comparison. This isn‚Äôt just boring, it‚Äôs boring <em>and</em> dumb. The plot is a mess, the dialogue is weak and action was so bad that a kid smashing action figures together would‚Äôve looked better. You can‚Äôt help but wonder if this thing was made as a tax write-off. Worst of all, the cast deserved better they were probably told this was going to be the next big Marvel event, only for it to be the next big joke. <strong>1/10</strong></p><p><hr></p><p>Well, that was that. Six hours in total, and only two of them were actually fun. Skip <em>Kraven</em> and <em>Madame Web</em>, just watch <em>Venom 3</em></p>",
    "rawMarkdown": "I ended up watching *Venom 3: The Last Dance*, *Kraven the Hunter*, and *Madame Web* back to back. Venom was fine, but the other two, oh my god. I‚Äôd heard the talk about how bad they were when they first released, but seeing them myself was something else entirely. Some of them don‚Äôt even warrant full reviews, so here are three short ones instead.\n\n---\n\n**Venom: The Last Dance (2024)**  \nIt‚Äôs dumb, but it‚Äôs sweet and somehow that‚Äôs exactly why it works. Probably the only Sony Spider-Man villain to actually make it through a full trilogy, Venom goes out on a surprisingly fun and heartfelt note. The plot is decent, a little messy in places, but never boring. The dancing scene with Mrs. Chen? Adorable. And Eddie saying goodbye to Venom with *Memories* by Maroon 5 playing in the background unintentionally hilarious, but still kind of touching. The Venom trilogy might and will be Sony‚Äôs best work: fun, ridiculous, but never dull. **10/10**\n\n---\n\n**Kraven the Hunter (2024)**  \nNot even ‚Äúfun bad,‚Äù just bad. Kraven feels like a Russian Tarzan knockoff, and not a good one. The twists are dumb, the characters are weak, and the whole thing drags on until you stop caring. Even cool-sounding characters like Chameleon, Foreigner, and Rhino are wasted and reduced to jokes. Completely forgettable. **1/10**\n\n---\n\n**Madame Web (2024)**  \nI made the mistake of watching this right after *Kraven the Hunter* and honestly, it made *Kraven* look like a masterpiece in comparison. This isn‚Äôt just boring, it‚Äôs boring *and* dumb. The plot is a mess, the dialogue is weak and action was so bad that a kid smashing action figures together would‚Äôve looked better. You can‚Äôt help but wonder if this thing was made as a tax write-off. Worst of all, the cast deserved better they were probably told this was going to be the next big Marvel event, only for it to be the next big joke. **1/10**\n\n---\n\nWell, that was that. Six hours in total, and only two of them were actually fun. Skip *Kraven* and *Madame Web*, just watch *Venom 3*"
  },
  {
    "id": "mini020920252144",
    "title": "A Stand Still : Lumex Project",
    "date": "2025-09-02",
    "time": "21:44",
    "tags": [
      "project",
      "computer vision",
      "ai"
    ],
    "content": "<p>Today I was working on my big project, a multimodal AI application. For this, I‚Äôm using a Luxonis OAK-D Pro, which combines a depth sensor and a camera. The idea is simple: it provides more data than a regular camera, which is why I bought one in the first place.</p><p>But the title <em>‚Äústand still‚Äù</em> sums up exactly where I am right now.</p><p>Last week, I thought I was routing the video stream directly to my phone. In reality, it was going from the camera ‚Üí PC ‚Üí phone. My first goal was a proper round trip, but I‚Äôve hit a literal stand still right at the beginning.</p><p>After digging deeper, I found out that I couldn‚Äôt just use a web server or web app to connect. The OAK-D needs a <strong>physical connection</strong> between the app and the camera. I looked into some GitHub repos that supposedly did this, but they‚Äôre all 4+ years old and broken. I even tried Chaquopy and a bunch of random approaches, hoping something would stick.</p><p>Finally, I stripped it all down and aimed for the bare minimum: just get the RGB stream working. After about three hours, I managed to build an app that _ran without crashing_. But it still couldn‚Äôt connect to the camera, and that‚Äôs where my progress stands.</p><p>Talking it over with others made me realize something important: I‚Äôve been shooting in the dark and just hoping things would work. That‚Äôs not uncommon, especially with how much AI and LLMs can ‚Äúfill in the blanks‚Äù for you. But at some point, the shortcuts collapse.</p><p>Now I see the real path forward: I need to break this down even further. I have to understand the <strong>protocols, pipelines, and data types</strong> at play. Only then will I actually be able to make this work.</p><p>All of the the failing, the learning, the breaking down happened in a single day. But I know I won‚Äôt feel fine until I get it running. For clarity (and maybe some sanity), I will keep updating my progress on Minis.</p>",
    "rawMarkdown": "Today I was working on my big project, a multimodal AI application. For this, I‚Äôm using a Luxonis OAK-D Pro, which combines a depth sensor and a camera. The idea is simple: it provides more data than a regular camera, which is why I bought one in the first place.\n\nBut the title *‚Äústand still‚Äù* sums up exactly where I am right now.\n\nLast week, I thought I was routing the video stream directly to my phone. In reality, it was going from the camera ‚Üí PC ‚Üí phone. My first goal was a proper round trip, but I‚Äôve hit a literal stand still right at the beginning.\n\nAfter digging deeper, I found out that I couldn‚Äôt just use a web server or web app to connect. The OAK-D needs a **physical connection** between the app and the camera. I looked into some GitHub repos that supposedly did this, but they‚Äôre all 4+ years old and broken. I even tried Chaquopy and a bunch of random approaches, hoping something would stick.\n\nFinally, I stripped it all down and aimed for the bare minimum: just get the RGB stream working. After about three hours, I managed to build an app that _ran without crashing_. But it still couldn‚Äôt connect to the camera, and that‚Äôs where my progress stands.\n\nTalking it over with others made me realize something important: I‚Äôve been shooting in the dark and just hoping things would work. That‚Äôs not uncommon, especially with how much AI and LLMs can ‚Äúfill in the blanks‚Äù for you. But at some point, the shortcuts collapse.\n\nNow I see the real path forward: I need to break this down even further. I have to understand the **protocols, pipelines, and data types** at play. Only then will I actually be able to make this work.\n\nAll of the the failing, the learning, the breaking down happened in a single day. But I know I won‚Äôt feel fine until I get it running. For clarity (and maybe some sanity), I will keep updating my progress on Minis."
  },
  {
    "id": "mini030920252254",
    "title": "A Realization : Lumex Project",
    "date": "2025-09-03",
    "time": "22:54",
    "tags": [
      "project",
      "computer vision",
      "ai",
      "software"
    ],
    "content": "<p>This is a continuation of me working on my big project, and I‚Äôve made some progress. I broke things down to the core basics of what I‚Äôm actually trying to do. After some deep digging (and a stressful back-and-forth with AI), I realized that the repo I was relying on which I already knew was 4 years old, was simply too flawed to sit and fix. I tried starting from scratch, but then I hit the real issue: <strong>AI itself.</strong></p><p>AI is good, no doubt. Without it, I wouldn‚Äôt have even thought of this project, let alone gotten this far without a complete breakdown. But the problem is, it just kept giving me the same outdated suggestions again and again.</p><p>If there‚Äôs one thing this project has taught me so far, it‚Äôs how much software can make your life miserable. Documentation is scarce, libraries change, support is thin, and when you‚Äôre trying to build something that hasn‚Äôt really been done before, it‚Äôs brutally difficult. The panic-filled sleeps and the dreams of failing have been haunting me since Sunday (31/8/25). Talking things over with my father, my accomplishments and failures alike has helped. Even if the problem doesn‚Äôt get solved, just talking about it helps. There‚Äôs really no one else I can share this with, so I‚Äôm grateful I at least have that.</p><p>Now, onto the progress. After going in circles, I finally snapped and told ChatGPT straight: <em>‚ÄúI‚Äôm running in loops here. I‚Äôm doing the same thing again and again. All your references are outdated ‚Äî 4 years old. Existing libraries don‚Äôt work due to changes. Stop giving me the same stuff and let‚Äôs actually think through how this can work.‚Äù</em></p><p>It paused for over four minutes and finally gave me the right direction. The solution: there <strong>does</strong> exist a Java API for DepthAI. It is : <a href='https://github.com/bytedeco/javacpp-presets/tree/master/depthai'>JAVA CPP DEPTHAI Preset</a> It was last updated just 10 months ago, and seeing it still being used in 2024 is a good sign. From there, I broke things down even further. Now my current version is basically just checking if the device is connected no OpenCV, nothing extra, just DepthAI.</p><p>After some back and forth with settings (and still battling the AI feeding me old information), I managed to create an APK that actually compiled the DepthAI library and built successfully. The only issue is that it was built under the assumption that the PC, phone, and OAK were all connected together. The APK ended up being around 400 MB, which isn‚Äôt a big deal, but now the next step is to strip it down: build a minimal UI and get this raw functionality working.</p><p>For now, I‚Äôve had to pause because of some external device issues, but I‚Äôll get back to it soon. In the meantime, I want to focus, reflect, and most importantly get my latest F1 recap out.</p><p>And that‚Äôs where my progress stands. If nothing else, I‚Äôve already learned that frustration and failure are just part of the process. The important thing is that I‚Äôm still moving, even when it feels like I‚Äôm standing still.</p>",
    "rawMarkdown": "This is a continuation of me working on my big project, and I‚Äôve made some progress. I broke things down to the core basics of what I‚Äôm actually trying to do. After some deep digging (and a stressful back-and-forth with AI), I realized that the repo I was relying on which I already knew was 4 years old, was simply too flawed to sit and fix. I tried starting from scratch, but then I hit the real issue: **AI itself.**\n\nAI is good, no doubt. Without it, I wouldn‚Äôt have even thought of this project, let alone gotten this far without a complete breakdown. But the problem is, it just kept giving me the same outdated suggestions again and again.\n\nIf there‚Äôs one thing this project has taught me so far, it‚Äôs how much software can make your life miserable. Documentation is scarce, libraries change, support is thin, and when you‚Äôre trying to build something that hasn‚Äôt really been done before, it‚Äôs brutally difficult. The panic-filled sleeps and the dreams of failing have been haunting me since Sunday (31/8/25). Talking things over with my father, my accomplishments and failures alike has helped. Even if the problem doesn‚Äôt get solved, just talking about it helps. There‚Äôs really no one else I can share this with, so I‚Äôm grateful I at least have that.\n\nNow, onto the progress. After going in circles, I finally snapped and told ChatGPT straight: *‚ÄúI‚Äôm running in loops here. I‚Äôm doing the same thing again and again. All your references are outdated ‚Äî 4 years old. Existing libraries don‚Äôt work due to changes. Stop giving me the same stuff and let‚Äôs actually think through how this can work.‚Äù*\n\nIt paused for over four minutes and finally gave me the right direction. The solution: there **does** exist a Java API for DepthAI. It is : (JAVA CPP DEPTHAI Preset)[https://github.com/bytedeco/javacpp-presets/tree/master/depthai] It was last updated just 10 months ago, and seeing it still being used in 2024 is a good sign. From there, I broke things down even further. Now my current version is basically just checking if the device is connected no OpenCV, nothing extra, just DepthAI.\n\nAfter some back and forth with settings (and still battling the AI feeding me old information), I managed to create an APK that actually compiled the DepthAI library and built successfully. The only issue is that it was built under the assumption that the PC, phone, and OAK were all connected together. The APK ended up being around 400 MB, which isn‚Äôt a big deal, but now the next step is to strip it down: build a minimal UI and get this raw functionality working.\n\nFor now, I‚Äôve had to pause because of some external device issues, but I‚Äôll get back to it soon. In the meantime, I want to focus, reflect, and most importantly get my latest F1 recap out.\n\nAnd that‚Äôs where my progress stands. If nothing else, I‚Äôve already learned that frustration and failure are just part of the process. The important thing is that I‚Äôm still moving, even when it feels like I‚Äôm standing still."
  },
  {
    "id": "mini11092025",
    "title": "Decent Progress : Lumex Project",
    "date": "2025-09-11",
    "time": "21:18",
    "tags": [
      "project",
      "computer vision",
      "ai",
      "software"
    ],
    "content": "<p><strong>Progress Update</strong><br>Over the past couple of days, I focused on getting the DepthAI AAR properly configured and integrated. For context, the AAR is essentially a packaged library (like a zip file) that contains the critical components of the DepthAI module, including the firmware and essential functionality.</p><p>Previously, I discovered that the Bytedeco C++ library worked with DepthAI but only supported the camera itself. not the USB connection. The publicly available AAR only supports USB. This led me to create a <strong>custom AAR</strong> that works for both the camera and USB.</p><p>The AAR build is now <strong>successful</strong>. Initially, there were compilation issues where only a single class file was being compiled while the others were skipped. After cleaning the Gradle cache, double-checking syntax, and following some debugging references, all class files are now correctly included in the AAR.</p><p>With the new AAR integrated, I managed to reach a working APK where both the USB connection and DepthAI function properly. Logs now show <strong>‚ÄúOAK connected‚Äù</strong> and <strong>‚ÄúPipeline started‚Äù</strong>, confirming that real communication is happening between the phone and the device and no dummy test is happening.</p><p><hr></p><p><strong>Current Challenge: RGB Streaming</strong><br>The next milestone is streaming RGB video to the phone. However, I ran into issues. likely due to a firmware mismatch. While communication works, the streaming pipeline isn‚Äôt functioning correctly. I attempted fixes but ended up in loops, so I‚Äôve paused this part for now.</p><p>Moving forward, I am rolling back to the <strong>pipeline-check checkpoint</strong> and rebuild from there, ensuring stability before moving onto streaming. I‚Äôm also moving the project to <strong>IntelliJ</strong>, as it‚Äôs suggested to be more efficient for this workflow compared to Android Studio. Some configuration issues came up, but I‚Äôm confident they‚Äôll be resolved soon.</p><p><hr></p><p><h3><strong>Reflections and Learnings</strong></h3></p><p>It‚Äôs been a week since I started this intensive phase of the project. effectively my second week. Progress isn‚Äôt measured by speed alone, the issues I‚Äôm encountering would take teams to resolve. Fortunately, I‚Äôm not alone in this my father has been assisting me, offering 35 years of software development experience. Talking to him gives me perspective and motivation when things feel overwhelming.</p><p>Mentally, I‚Äôm stable, but there are moments of fear and self-doubt that made it hard to open Android Studio some days. Even when I make progress, my heart races, and I feel momentarily lost. Yet, I‚Äôm still moving forward, which feels like a core part of me saying, <em>‚ÄúThe only way forward is through.‚Äù</em></p><p>This project is changing me in ways I don‚Äôt fully understand yet. I know that taking breaks is important, but equally important is coming back from them with determination, rather than letting fear take hold. Despite challenges, I feel a quiet confidence that this journey is worth every bit of effort.</p>",
    "rawMarkdown": "**Progress Update**  \nOver the past couple of days, I focused on getting the DepthAI AAR properly configured and integrated. For context, the AAR is essentially a packaged library (like a zip file) that contains the critical components of the DepthAI module, including the firmware and essential functionality.\n\nPreviously, I discovered that the Bytedeco C++ library worked with DepthAI but only supported the camera itself. not the USB connection. The publicly available AAR only supports USB. This led me to create a **custom AAR** that works for both the camera and USB.\n\nThe AAR build is now **successful**. Initially, there were compilation issues where only a single class file was being compiled while the others were skipped. After cleaning the Gradle cache, double-checking syntax, and following some debugging references, all class files are now correctly included in the AAR.\n\nWith the new AAR integrated, I managed to reach a working APK where both the USB connection and DepthAI function properly. Logs now show **‚ÄúOAK connected‚Äù** and **‚ÄúPipeline started‚Äù**, confirming that real communication is happening between the phone and the device and no dummy test is happening.\n\n---\n\n**Current Challenge: RGB Streaming**  \nThe next milestone is streaming RGB video to the phone. However, I ran into issues. likely due to a firmware mismatch. While communication works, the streaming pipeline isn‚Äôt functioning correctly. I attempted fixes but ended up in loops, so I‚Äôve paused this part for now.\n\nMoving forward, I am rolling back to the **pipeline-check checkpoint** and rebuild from there, ensuring stability before moving onto streaming. I‚Äôm also moving the project to **IntelliJ**, as it‚Äôs suggested to be more efficient for this workflow compared to Android Studio. Some configuration issues came up, but I‚Äôm confident they‚Äôll be resolved soon.\n\n---\n\n### **Reflections and Learnings**\n\nIt‚Äôs been a week since I started this intensive phase of the project. effectively my second week. Progress isn‚Äôt measured by speed alone, the issues I‚Äôm encountering would take teams to resolve. Fortunately, I‚Äôm not alone in this my father has been assisting me, offering 35 years of software development experience. Talking to him gives me perspective and motivation when things feel overwhelming.\n\nMentally, I‚Äôm stable, but there are moments of fear and self-doubt that made it hard to open Android Studio some days. Even when I make progress, my heart races, and I feel momentarily lost. Yet, I‚Äôm still moving forward, which feels like a core part of me saying, *‚ÄúThe only way forward is through.‚Äù*\n\nThis project is changing me in ways I don‚Äôt fully understand yet. I know that taking breaks is important, but equally important is coming back from them with determination, rather than letting fear take hold. Despite challenges, I feel a quiet confidence that this journey is worth every bit of effort."
  },
  {
    "id": "mini30112025",
    "title": "The Problem is Me (and So Is the Solution)",
    "date": "2025-11-30",
    "time": "23:52",
    "tags": [
      "blog",
      "project",
      "growth"
    ],
    "content": "<p>I‚Äôm finally back with a live blog, it‚Äôs been a while. I‚Äôve still been writing: show reviews, bits about life, and recently, that whole journey with the DepthAI project. But this one hits differently. This one comes after getting hit with a brick. Not literally‚Ä¶ though honestly, that might feel easier than what this past quarter has been.</p><p>When I look back at the last three months, I‚Äôve been carrying the entire software world‚Äôs sins on my shoulders. If Android has an error, <strong>I</strong> feel broken. My dad told me straight: I need to stop taking every bug personally. And he‚Äôs right, I‚Äôve tied every failure to who I am. It‚Äôs gotten bad enough that I don‚Äôt work because I fear either succeeding‚Ä¶ or failing again.</p><p>What is worse? I chase errors now. Because if I find one, I get a free pass to delay everything till the next big disaster. And I‚Äôve been doing exactly that.</p><p>I‚Äôm scared. <strong>Really</strong> scared. And it‚Äôs not like I‚Äôm behind, everything is up to date, files ready, environment set. One click to build and I still backed away.</p><p>If I keep running from problems like this, why am I even in software? This is a crisis of confidence, plain and simple, It's the worry that people will think less of me‚Ä¶ or worse, that <strong>I</strong> will.</p><p>There will always be problems. That‚Äôs the job. But once you solve them, they‚Äôre gone. I don‚Äôt have to carry them forever.</p><p>So I‚Äôm leaving all that weight behind. Not looking back. Just driving forward ‚Äî fast, focused, relentless until I get tired. And even when I do, I‚Äôll rest without regret. As Elsa said \"Let It Go!\"</p><p>It‚Äôs a new month, the final stretch of the year. I‚Äôm not waiting for signs or motivation or fate to approve my progress. I‚Äôm done with that.</p><p>I‚Äôm capable. I‚Äôve got this.</p><p>And yes ‚Äî expect the DepthAI project to be up and running very soon.</p>",
    "rawMarkdown": "I‚Äôm finally back with a live blog, it‚Äôs been a while. I‚Äôve still been writing: show reviews, bits about life, and recently, that whole journey with the DepthAI project. But this one hits differently. This one comes after getting hit with a brick. Not literally‚Ä¶ though honestly, that might feel easier than what this past quarter has been.\n\nWhen I look back at the last three months, I‚Äôve been carrying the entire software world‚Äôs sins on my shoulders. If Android has an error, **I** feel broken. My dad told me straight: I need to stop taking every bug personally. And he‚Äôs right, I‚Äôve tied every failure to who I am. It‚Äôs gotten bad enough that I don‚Äôt work because I fear either succeeding‚Ä¶ or failing again.\n\nWhat is worse? I chase errors now. Because if I find one, I get a free pass to delay everything till the next big disaster. And I‚Äôve been doing exactly that.\n\nI‚Äôm scared. **Really** scared. And it‚Äôs not like I‚Äôm behind, everything is up to date, files ready, environment set. One click to build and I still backed away.\n\nIf I keep running from problems like this, why am I even in software? This is a crisis of confidence, plain and simple, It's the worry that people will think less of me‚Ä¶ or worse, that **I** will.\n\nThere will always be problems. That‚Äôs the job. But once you solve them, they‚Äôre gone. I don‚Äôt have to carry them forever.\n\nSo I‚Äôm leaving all that weight behind. Not looking back. Just driving forward ‚Äî fast, focused, relentless until I get tired. And even when I do, I‚Äôll rest without regret. As Elsa said \"Let It Go!\"\n\nIt‚Äôs a new month, the final stretch of the year. I‚Äôm not waiting for signs or motivation or fate to approve my progress. I‚Äôm done with that.\n\nI‚Äôm capable. I‚Äôve got this.\n\nAnd yes ‚Äî expect the DepthAI project to be up and running very soon."
  },
  {
    "id": "mini13012026",
    "title": "The Framework Dilema",
    "date": "2026-01-13",
    "time": "11:19",
    "tags": [
      "blog",
      "project",
      "experience"
    ],
    "content": "<p>I'm working on a project that involves running a YOLO model on the web using <strong>ONNX Runtime Web</strong>. On paper, it sounded straightforward: load the model, run inference, detect people. In practice, it turned into one of the most educational (and humbling) experiences I‚Äôve had so far.</p><p>I started the way I usually do with personal projects, dive in headfirst. I planned just enough to get moving, implemented things quickly, fixed errors as they came up, and tried to squeeze out as much performance as possible. This approach has always worked for me. And honestly, there‚Äôs nothing wrong with it.</p><p>But this project was different.</p><p>This wasn‚Äôt just about ‚Äúmaking it fast.‚Äù It was about understanding <strong>why it wasn‚Äôt fast</strong> and more importantly, how performance is actually measured and reasoned about in real-world software.</p><p><hr></p><p><h2>When Numbers Don‚Äôt Make Sense</h2></p><p>My initial inference time was around <strong>5000 ms</strong>. That‚Äôs bad. Detection lagged, the experience felt broken, and no amount of surface-level tweaking seemed to help. My instinct was to assume something was wrong with ONNX, the model, or even the browser.</p><p>That‚Äôs when I was taught something crucial. You map everything out. Every step. Every call. Every dependency.</p><p>That sounds obvious but it‚Äôs genuinely hard when you‚Äôve never worked on a client-facing project before. For the first time, I wasn‚Äôt just optimizing for myself. I was dealing with <strong>real constraints, real expectations, and real people</strong>. That pressure changes how you think.</p><p>Once I slowed down and actually mapped out the pipeline, things began to click. And honestly, I was more annoyed at myself than anything else because the answer was right there.</p><p><hr></p><p><h2>Vanilla JS vs Frameworks: The Real Difference</h2></p><p>To isolate the issue, I compared my implementation with another project running the <strong>same YOLOv12n model</strong>, doing the same task (person detection). That project used <strong>Next.js</strong>. Their inference time? Around <strong>150 ms</strong>.</p><p>Mine? Still stuck around <strong>1500 ms</strong>. The difference wasn‚Äôt the model.<br>It wasn‚Äôt ONNX. It wasn‚Äôt even the browser.</p><p>It was <strong>how things were loaded and orchestrated</strong>.</p><p>In my vanilla JavaScript setup, libraries were pulled in via CDN in a largely sequential manner. Scripts loaded step-by-step, dependencies resolved one after another, and the runtime paid the price.</p><p>Frameworks like Next.js, on the other hand, do a _lot_ of invisible work for you:</p><p>- Asynchronous and parallel loading of packages<br>- Smarter bundling and dependency resolution<br>- Optimized preprocessing pipelines<br>- Runtime optimizations that you don‚Äôt even think about until you miss them    </p><p>Once I understood this, the performance gap stopped feeling mysterious and started feeling inevitable.</p><p><hr></p><p><h2>What This Taught Me About Frameworks</h2></p><p>We throw around the word _framework_ all the time. ‚ÄúUse React.‚Äù ‚ÄúUse Next.‚Äù ‚ÄúFrameworks make things faster.‚Äù</p><p>But <strong>why</strong>? This project finally answered that for me.</p><p>Frameworks aren‚Äôt magic. They‚Äôre distilled experience. They encode years of lessons about loading strategies, execution order, caching, and runtime behavior things that are incredibly hard to get right on your own unless you‚Äôve already made every mistake once.</p><p>Once you understand _what_ a framework is actually doing for you, adapting to new tools becomes much easier. You stop treating them as black boxes and start seeing them as systems with advantages and trade-offs.</p><p><hr></p><p><h2>The Bigger Takeaway</h2></p><p>This experience taught me a lot about Development vs production thinking. About how you make something just to work vs making it reliable and more importantly it taught the consequences of optimizing blindy instead of with intent.   </p><p>It‚Äôs not the end of this project. But now I know it‚Äôs not about randomly optimizing or blaming the model. It‚Äôs about understanding the full pipeline: how things load, when they execute, and where time is actually spent.</p><p>Once that clicks, the problem stops feeling chaotic. Performance becomes something I can reason about, test systematically, and improve with intent.</p><p>To better understand everything end-to-end, I used <strong>NotebookLM</strong>, and it turned out to be genuinely enlightening.</p><p>If you‚Äôre curious, check it out here: üëâ <a href=\"https://notebooklm.google.com/notebook/ee800966-4605-4691-8492-79f27bc89f7c\" target=\"_blank\" rel=\"noopener noreferrer\">NotebookLM</a></p>",
    "rawMarkdown": "I'm working on a project that involves running a YOLO model on the web using **ONNX Runtime Web**. On paper, it sounded straightforward: load the model, run inference, detect people. In practice, it turned into one of the most educational (and humbling) experiences I‚Äôve had so far.\n\nI started the way I usually do with personal projects, dive in headfirst. I planned just enough to get moving, implemented things quickly, fixed errors as they came up, and tried to squeeze out as much performance as possible. This approach has always worked for me. And honestly, there‚Äôs nothing wrong with it.\n\nBut this project was different.\n\nThis wasn‚Äôt just about ‚Äúmaking it fast.‚Äù It was about understanding **why it wasn‚Äôt fast** and more importantly, how performance is actually measured and reasoned about in real-world software.\n\n---\n\n## When Numbers Don‚Äôt Make Sense\n\nMy initial inference time was around **5000 ms**. That‚Äôs bad. Detection lagged, the experience felt broken, and no amount of surface-level tweaking seemed to help. My instinct was to assume something was wrong with ONNX, the model, or even the browser.\n\nThat‚Äôs when I was taught something crucial. You map everything out. Every step. Every call. Every dependency.\n\nThat sounds obvious but it‚Äôs genuinely hard when you‚Äôve never worked on a client-facing project before. For the first time, I wasn‚Äôt just optimizing for myself. I was dealing with **real constraints, real expectations, and real people**. That pressure changes how you think.\n\nOnce I slowed down and actually mapped out the pipeline, things began to click. And honestly, I was more annoyed at myself than anything else because the answer was right there.\n\n---\n\n## Vanilla JS vs Frameworks: The Real Difference\n\nTo isolate the issue, I compared my implementation with another project running the **same YOLOv12n model**, doing the same task (person detection). That project used **Next.js**. Their inference time? Around **150 ms**.\n\nMine? Still stuck around **1500 ms**. The difference wasn‚Äôt the model.  \nIt wasn‚Äôt ONNX. It wasn‚Äôt even the browser.\n\nIt was **how things were loaded and orchestrated**.\n\nIn my vanilla JavaScript setup, libraries were pulled in via CDN in a largely sequential manner. Scripts loaded step-by-step, dependencies resolved one after another, and the runtime paid the price.\n\nFrameworks like Next.js, on the other hand, do a _lot_ of invisible work for you:\n\n- Asynchronous and parallel loading of packages    \n- Smarter bundling and dependency resolution    \n- Optimized preprocessing pipelines    \n- Runtime optimizations that you don‚Äôt even think about until you miss them    \n\nOnce I understood this, the performance gap stopped feeling mysterious and started feeling inevitable.\n\n---\n\n## What This Taught Me About Frameworks\n\nWe throw around the word _framework_ all the time. ‚ÄúUse React.‚Äù ‚ÄúUse Next.‚Äù ‚ÄúFrameworks make things faster.‚Äù\n\nBut **why**? This project finally answered that for me.\n\nFrameworks aren‚Äôt magic. They‚Äôre distilled experience. They encode years of lessons about loading strategies, execution order, caching, and runtime behavior things that are incredibly hard to get right on your own unless you‚Äôve already made every mistake once.\n\nOnce you understand _what_ a framework is actually doing for you, adapting to new tools becomes much easier. You stop treating them as black boxes and start seeing them as systems with advantages and trade-offs.\n\n---\n\n## The Bigger Takeaway\n\nThis experience taught me a lot about Development vs production thinking. About how you make something just to work vs making it reliable and more importantly it taught the consequences of optimizing blindy instead of with intent.   \n\nIt‚Äôs not the end of this project. But now I know it‚Äôs not about randomly optimizing or blaming the model. It‚Äôs about understanding the full pipeline: how things load, when they execute, and where time is actually spent.\n\nOnce that clicks, the problem stops feeling chaotic. Performance becomes something I can reason about, test systematically, and improve with intent.\n\nTo better understand everything end-to-end, I used **NotebookLM**, and it turned out to be genuinely enlightening.\n\nIf you‚Äôre curious, check it out here: üëâ [NotebookLM](https://notebooklm.google.com/notebook/ee800966-4605-4691-8492-79f27bc89f7c)"
  }
]
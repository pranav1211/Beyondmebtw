<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Real-Time Object Detection - Vanilla JS</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: Arial, sans-serif;
            background: #1a1a1a;
            color: #fff;
            padding: 20px;
            display: flex;
            flex-direction: column;
            align-items: center;
            min-height: 100vh;
        }

        h1 {
            margin-bottom: 20px;
            font-size: 28px;
        }

        #loading {
            font-size: 18px;
            margin: 20px;
        }

        #app {
            display: none;
            width: 100%;
            max-width: 1400px;
        }

        #app.active {
            display: flex;
            flex-direction: column;
            align-items: center;
        }

        #webcam-container {
            position: relative;
            display: inline-block;
            margin: 20px 0;
        }

        #webcam {
            display: block;
            max-width: 100%;
            height: auto;
        }

        #canvas {
            position: absolute;
            top: 0;
            left: 0;
            pointer-events: none;
        }

        .controls {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            justify-content: center;
            margin: 20px 0;
        }

        button, select {
            padding: 12px 24px;
            font-size: 16px;
            border: 2px dashed #666;
            background: #2a2a2a;
            color: #fff;
            border-radius: 12px;
            cursor: pointer;
            transition: transform 0.2s;
        }

        button:hover, select:hover {
            transform: translateY(-2px);
            border-color: #888;
        }

        button:active {
            transform: translateY(0);
        }

        button.active {
            background: #fff;
            color: #000;
        }

        select {
            min-width: 200px;
        }

        .stats {
            display: flex;
            flex-wrap: wrap;
            gap: 30px;
            justify-content: space-around;
            width: 100%;
            padding: 20px;
            background: #2a2a2a;
            border-radius: 12px;
            margin: 20px 0;
        }

        .stat-group {
            text-align: center;
        }

        .stat-group h3 {
            font-size: 14px;
            color: #888;
            margin-bottom: 10px;
        }

        .stat-value {
            font-size: 24px;
            font-weight: bold;
            color: #4CAF50;
        }

        .model-info {
            font-size: 18px;
            margin: 10px 0;
            color: #888;
        }

        .error {
            background: #d32f2f;
            color: white;
            padding: 15px;
            border-radius: 8px;
            margin: 20px 0;
        }
    </style>
</head>
<body>
    <h1>Real-Time Object Detection - Vanilla JS</h1>

    <div id="loading">Loading ONNX Runtime...</div>

    <div id="app">
        <div id="webcam-container">
            <video id="webcam" autoplay playsinline></video>
            <canvas id="canvas"></canvas>
        </div>

        <div class="model-info">
            Model: <span id="current-model">Loading...</span>
        </div>

        <div class="controls">
            <select id="model-select">
                <option value="0">YOLOv12n - 256x256</option>
                <option value="1">YOLOv11n - 256x256</option>
                <option value="2">YOLOv10n - 256x256</option>
                <option value="3">YOLOv7-tiny - 256x256</option>
                <option value="4">YOLOv7-tiny - 320x320</option>
                <option value="5">YOLOv7-tiny - 640x640</option>
            </select>
            <button id="capture-btn">Capture Photo</button>
            <button id="live-btn">Live Detection</button>
            <button id="switch-camera-btn">Switch Camera</button>
            <button id="reset-btn">Reset</button>
        </div>

        <div class="stats">
            <div class="stat-group">
                <h3>Inference Time</h3>
                <div class="stat-value" id="inference-time">0ms</div>
            </div>
            <div class="stat-group">
                <h3>Total Time</h3>
                <div class="stat-value" id="total-time">0ms</div>
            </div>
            <div class="stat-group">
                <h3>Overhead</h3>
                <div class="stat-value" id="overhead-time">0ms</div>
            </div>
            <div class="stat-group">
                <h3>Model FPS</h3>
                <div class="stat-value" id="model-fps">0</div>
            </div>
            <div class="stat-group">
                <h3>Total FPS</h3>
                <div class="stat-value" id="total-fps">0</div>
            </div>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.13.1/dist/ort.min.js"></script>
    <script>
        // YOLO Classes (COCO dataset)
        const yoloClasses = [
            "person", "bicycle", "car", "motorbike", "aeroplane", "bus", "train", "truck", "boat",
            "traffic light", "fire hydrant", "stop sign", "parking meter", "bench", "bird", "cat",
            "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra", "giraffe", "backpack",
            "umbrella", "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard", "sports ball",
            "kite", "baseball bat", "baseball glove", "skateboard", "surfboard", "tennis racket",
            "bottle", "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana", "apple",
            "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake", "chair",
            "sofa", "pottedplant", "bed", "diningtable", "toilet", "tvmonitor", "laptop", "mouse",
            "remote", "keyboard", "cell phone", "microwave", "oven", "toaster", "sink",
            "refrigerator", "book", "clock", "vase", "scissors", "teddy bear", "hair drier", "toothbrush"
        ];

        // Model configurations
        const MODEL_CONFIGS = [
            { resolution: [256, 256], name: 'yolo12n.onnx' },
            { resolution: [256, 256], name: 'yolo11n.onnx' },
            { resolution: [256, 256], name: 'yolov10n.onnx' },
            { resolution: [256, 256], name: 'yolov7-tiny_256x256.onnx' },
            { resolution: [320, 320], name: 'yolov7-tiny_320x320.onnx' },
            { resolution: [640, 640], name: 'yolov7-tiny_640x640.onnx' }
        ];

        // App state
        let session = null;
        let currentModelIndex = 0;
        let currentModelConfig = MODEL_CONFIGS[0];
        let liveDetectionActive = false;
        let facingMode = 'environment';
        let stream = null;

        // DOM elements
        const loadingEl = document.getElementById('loading');
        const appEl = document.getElementById('app');
        const videoEl = document.getElementById('webcam');
        const canvasEl = document.getElementById('canvas');
        const ctx = canvasEl.getContext('2d', { willReadFrequently: true });
        const modelSelectEl = document.getElementById('model-select');
        const captureBtnEl = document.getElementById('capture-btn');
        const liveBtnEl = document.getElementById('live-btn');
        const switchCameraBtnEl = document.getElementById('switch-camera-btn');
        const resetBtnEl = document.getElementById('reset-btn');
        const currentModelEl = document.getElementById('current-model');
        const inferenceTimeEl = document.getElementById('inference-time');
        const totalTimeEl = document.getElementById('total-time');
        const overheadTimeEl = document.getElementById('overhead-time');
        const modelFpsEl = document.getElementById('model-fps');
        const totalFpsEl = document.getElementById('total-fps');

        // Initialize ONNX Runtime
        async function initONNX() {
            try {
                // Set ONNX Runtime to use WASM
                ort.env.wasm.numThreads = 1;
                ort.env.wasm.simd = true;

                loadingEl.textContent = 'Loading model...';
                await loadModel(currentModelIndex);

                loadingEl.textContent = 'Starting camera...';
                await startCamera();

                loadingEl.style.display = 'none';
                appEl.classList.add('active');

                updateCanvasSize();
            } catch (error) {
                loadingEl.innerHTML = `<div class="error">Error: ${error.message}</div>`;
                console.error('Initialization error:', error);
            }
        }

        // Load ONNX model
        async function loadModel(modelIndex) {
            try {
                currentModelIndex = modelIndex;
                currentModelConfig = MODEL_CONFIGS[modelIndex];

                const modelPath = `models/${currentModelConfig.name}`;
                currentModelEl.textContent = `${currentModelConfig.name} (${currentModelConfig.resolution.join('x')})`;

                session = await ort.InferenceSession.create(modelPath, {
                    executionProviders: ['wasm'],
                    graphOptimizationLevel: 'all'
                });

                console.log('Model loaded:', currentModelConfig.name);
            } catch (error) {
                console.error('Error loading model:', error);
                throw new Error(`Failed to load model: ${error.message}`);
            }
        }

        // Start camera
        async function startCamera() {
            try {
                if (stream) {
                    stream.getTracks().forEach(track => track.stop());
                }

                const constraints = {
                    video: {
                        facingMode: facingMode,
                        width: { ideal: 1280 },
                        height: { ideal: 720 }
                    },
                    audio: false
                };

                stream = await navigator.mediaDevices.getUserMedia(constraints);
                videoEl.srcObject = stream;

                return new Promise((resolve) => {
                    videoEl.onloadedmetadata = () => {
                        updateCanvasSize();
                        resolve();
                    };
                });
            } catch (error) {
                throw new Error(`Camera access denied: ${error.message}`);
            }
        }

        // Update canvas size to match video
        function updateCanvasSize() {
            canvasEl.width = videoEl.offsetWidth;
            canvasEl.height = videoEl.offsetHeight;
        }

        // Preprocess image for YOLO
        function preprocess() {
            const [width, height] = currentModelConfig.resolution;

            // Create temporary canvas for resizing
            const tempCanvas = document.createElement('canvas');
            tempCanvas.width = width;
            tempCanvas.height = height;
            const tempCtx = tempCanvas.getContext('2d');

            // Draw and resize video frame
            if (facingMode === 'user') {
                tempCtx.translate(width, 0);
                tempCtx.scale(-1, 1);
            }
            tempCtx.drawImage(videoEl, 0, 0, width, height);

            // Get image data
            const imageData = tempCtx.getImageData(0, 0, width, height);
            const { data } = imageData;

            // Convert to RGB and normalize
            const input = new Float32Array(width * height * 3);

            for (let i = 0; i < width * height; i++) {
                input[i] = data[i * 4] / 255.0;                    // R
                input[width * height + i] = data[i * 4 + 1] / 255.0;     // G
                input[width * height * 2 + i] = data[i * 4 + 2] / 255.0; // B
            }

            return new ort.Tensor('float32', input, [1, 3, width, height]);
        }

        // Run model inference
        async function runInference() {
            const inputTensor = preprocess();
            const feeds = {};
            feeds[session.inputNames[0]] = inputTensor;

            const start = performance.now();
            const results = await session.run(feeds);
            const inferenceTime = performance.now() - start;

            const output = results[session.outputNames[0]];
            return { output, inferenceTime };
        }

        // Confidence to color
        function conf2color(conf) {
            const r = Math.round(255 * (1 - conf));
            const g = Math.round(255 * conf);
            return `rgb(${r},${g},0)`;
        }

        // Calculate IoU (Intersection over Union)
        function calculateIoU(boxA, boxB) {
            const x0 = Math.max(boxA.x0, boxB.x0);
            const y0 = Math.max(boxA.y0, boxB.y0);
            const x1 = Math.min(boxA.x1, boxB.x1);
            const y1 = Math.min(boxA.y1, boxB.y1);

            const intersectionArea = Math.max(0, x1 - x0) * Math.max(0, y1 - y0);

            const boxAArea = (boxA.x1 - boxA.x0) * (boxA.y1 - boxA.y0);
            const boxBArea = (boxB.x1 - boxB.x0) * (boxB.y1 - boxB.y0);
            const unionArea = boxAArea + boxBArea - intersectionArea;

            return intersectionArea / unionArea;
        }

        // Apply Non-Maximum Suppression
        function applyNMS(detections, iouThreshold) {
            detections.sort((a, b) => b.confidence - a.confidence);

            const keep = new Array(detections.length).fill(true);

            for (let i = 0; i < detections.length; i++) {
                if (!keep[i]) continue;

                const boxA = detections[i];
                for (let j = i + 1; j < detections.length; j++) {
                    if (!keep[j]) continue;

                    const boxB = detections[j];

                    if (boxA.classId !== boxB.classId) continue;

                    const iou = calculateIoU(boxA, boxB);

                    if (iou > iouThreshold) {
                        keep[j] = false;
                    }
                }
            }

            return detections.filter((_, index) => keep[index]);
        }

        // Postprocess YOLOv11/v12
        function postprocessYolov11(outputTensor) {
            const [width, height] = currentModelConfig.resolution;
            const dx = canvasEl.width / width;
            const dy = canvasEl.height / height;

            const numClasses = 80;
            const numAnchors = outputTensor.dims[2];
            const confidenceThreshold = 0.25;

            const detections = [];

            for (let i = 0; i < numAnchors; i++) {
                const x_center = outputTensor.data[i];
                const y_center = outputTensor.data[numAnchors + i];
                const w = outputTensor.data[2 * numAnchors + i];
                const h = outputTensor.data[3 * numAnchors + i];

                let maxClassScore = 0;
                let maxClassId = 0;

                for (let j = 0; j < numClasses; j++) {
                    const classScore = outputTensor.data[(4 + j) * numAnchors + i];
                    if (classScore > maxClassScore) {
                        maxClassScore = classScore;
                        maxClassId = j;
                    }
                }

                if (maxClassScore > confidenceThreshold) {
                    const x0 = x_center - w / 2;
                    const y0 = y_center - h / 2;
                    const x1 = x_center + w / 2;
                    const y1 = y_center + h / 2;

                    detections.push({
                        x0: x0 * dx,
                        y0: y0 * dy,
                        x1: x1 * dx,
                        y1: y1 * dy,
                        confidence: maxClassScore,
                        classId: maxClassId
                    });
                }
            }

            return applyNMS(detections, 0.4);
        }

        // Postprocess YOLOv10
        function postprocessYolov10(outputTensor) {
            const [width, height] = currentModelConfig.resolution;
            const dx = canvasEl.width / width;
            const dy = canvasEl.height / height;

            const detections = [];

            for (let i = 0; i < outputTensor.dims[1]; i++) {
                const offset = i * 6;
                const x0 = outputTensor.data[offset];
                const y0 = outputTensor.data[offset + 1];
                const x1 = outputTensor.data[offset + 2];
                const y1 = outputTensor.data[offset + 3];
                const score = outputTensor.data[offset + 4];
                const cls_id = outputTensor.data[offset + 5];

                if (score < 0.25) break;

                detections.push({
                    x0: x0 * dx,
                    y0: y0 * dy,
                    x1: x1 * dx,
                    y1: y1 * dy,
                    confidence: score,
                    classId: Math.round(cls_id)
                });
            }

            return detections;
        }

        // Postprocess YOLOv7
        function postprocessYolov7(outputTensor) {
            const [width, height] = currentModelConfig.resolution;
            const dx = canvasEl.width / width;
            const dy = canvasEl.height / height;

            const detections = [];

            for (let i = 0; i < outputTensor.dims[0]; i++) {
                const offset = i * 7;
                const batch_id = outputTensor.data[offset];
                const x0 = outputTensor.data[offset + 1];
                const y0 = outputTensor.data[offset + 2];
                const x1 = outputTensor.data[offset + 3];
                const y1 = outputTensor.data[offset + 4];
                const cls_id = outputTensor.data[offset + 5];
                const score = outputTensor.data[offset + 6];

                detections.push({
                    x0: x0 * dx,
                    y0: y0 * dy,
                    x1: x1 * dx,
                    y1: y1 * dy,
                    confidence: score,
                    classId: Math.round(cls_id)
                });
            }

            return detections;
        }

        // Postprocess based on model
        function postprocess(outputTensor) {
            const modelName = currentModelConfig.name;

            if (modelName.includes('yolo12') || modelName.includes('yolo11')) {
                return postprocessYolov11(outputTensor);
            } else if (modelName.includes('yolov10')) {
                return postprocessYolov10(outputTensor);
            } else if (modelName.includes('yolov7')) {
                return postprocessYolov7(outputTensor);
            }

            return [];
        }

        // Draw detections on canvas
        function drawDetections(detections) {
            ctx.clearRect(0, 0, canvasEl.width, canvasEl.height);

            for (const det of detections) {
                const { x0, y0, x1, y1, confidence, classId } = det;

                const score = Math.round(confidence * 100 * 10) / 10;
                const className = yoloClasses[classId];
                const label = `${className.charAt(0).toUpperCase() + className.slice(1)} ${score}%`;
                const color = conf2color(confidence);

                // Draw box
                ctx.strokeStyle = color;
                ctx.lineWidth = 3;
                ctx.strokeRect(x0, y0, x1 - x0, y1 - y0);

                // Draw filled box with transparency
                ctx.fillStyle = color.replace(')', ', 0.2)').replace('rgb', 'rgba');
                ctx.fillRect(x0, y0, x1 - x0, y1 - y0);

                // Draw label
                ctx.font = '20px Arial';
                ctx.fillStyle = color;
                ctx.fillText(label, x0, y0 - 5);
            }
        }

        // Update stats display
        function updateStats(inferenceTime, totalTime) {
            const overheadTime = totalTime - inferenceTime;
            const modelFps = 1000 / inferenceTime;
            const totalFps = 1000 / totalTime;

            inferenceTimeEl.textContent = `${Math.round(inferenceTime)}ms`;
            totalTimeEl.textContent = `${Math.round(totalTime)}ms`;
            overheadTimeEl.textContent = `+${overheadTime.toFixed(2)}ms`;
            modelFpsEl.textContent = `${modelFps.toFixed(2)}`;
            totalFpsEl.textContent = `${totalFps.toFixed(2)}`;
        }

        // Capture single frame
        async function captureFrame() {
            const totalStart = performance.now();

            const { output, inferenceTime } = await runInference();
            const detections = postprocess(output);
            drawDetections(detections);

            const totalTime = performance.now() - totalStart;
            updateStats(inferenceTime, totalTime);
        }

        // Live detection loop
        async function runLiveDetection() {
            while (liveDetectionActive) {
                await captureFrame();
                await new Promise(resolve => requestAnimationFrame(resolve));
            }
        }

        // Reset canvas
        function resetCanvas() {
            ctx.clearRect(0, 0, canvasEl.width, canvasEl.height);
            liveDetectionActive = false;
            liveBtnEl.classList.remove('active');
        }

        // Event listeners
        modelSelectEl.addEventListener('change', async (e) => {
            resetCanvas();
            loadingEl.style.display = 'block';
            loadingEl.textContent = 'Loading new model...';
            appEl.classList.remove('active');

            await loadModel(parseInt(e.target.value));

            loadingEl.style.display = 'none';
            appEl.classList.add('active');
        });

        captureBtnEl.addEventListener('click', () => {
            liveDetectionActive = false;
            liveBtnEl.classList.remove('active');
            captureFrame();
        });

        liveBtnEl.addEventListener('click', () => {
            liveDetectionActive = !liveDetectionActive;
            liveBtnEl.classList.toggle('active');

            if (liveDetectionActive) {
                runLiveDetection();
            }
        });

        switchCameraBtnEl.addEventListener('click', async () => {
            resetCanvas();
            facingMode = facingMode === 'user' ? 'environment' : 'user';
            await startCamera();
        });

        resetBtnEl.addEventListener('click', () => {
            resetCanvas();
        });

        // Handle window resize
        window.addEventListener('resize', () => {
            updateCanvasSize();
        });

        // Start the app
        window.addEventListener('load', initONNX);

        // Cleanup on page unload
        window.addEventListener('beforeunload', () => {
            if (stream) {
                stream.getTracks().forEach(track => track.stop());
            }
        });
    </script>
</body>
</html>
